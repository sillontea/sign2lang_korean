{"cells":[{"cell_type":"markdown","source":["# 사전 세팅\n","- 필요한 라이브러리를 import 합니다.\n","- 모델 구현 함수를 작성합니다.\n","- id 정보를 레이블(한글)로 바꿔줄 함수를 작성합니다.(id_to_label)\n","- colab에서 video를 사용하기 위한 코드를 작성합니다."],"metadata":{"id":"PrBhklXdErOy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cO3PkFj4mNSt"},"outputs":[],"source":["# https://tutorials.pytorch.kr/intermediate/torchvision_tutorial.html\n","# https://hyungjobyun.github.io/machinelearning/FasterRCNN2/\n","# 다음을 참조: https://github.com/HyungjoByun/Projects/blob/main/Faster%20RCNN/Faster_RCNN_Test.ipynb\n","    \n","import os\n","# os.environ['PYTORCH_ENABLE_MPS_FALLBACK']='1'\n","\n","# Image drawing\n","import cv2  # open cv\n","from PIL import Image\n","from PIL import ImageFont\n","from PIL import ImageDraw\n","import matplotlib.pyplot as plt\n","\n","# pytorch\n","import torch             \n","import torch.nn as nn     #pytorch network\n","import torch.optim as optim         #pytorch optimizer\n","\n","import torch.utils.data\n","from torch.utils.data import Dataset, DataLoader \n","\n","import torchvision                  #torchvision\n","from torchvision import transforms as T  #torchvision transform\n","\n","# Image augmentation \n","import imgaug as ia        #imgaug\n","from imgaug import augmenters as iaa\n","\n","# tensorboard(training status check)\n","import torch.utils.tensorboard as tensorboard\n","from torch.utils.tensorboard import SummaryWriter    \n","\n","# xml parsing\n","import time\n","import xml.etree.ElementTree as Et      \n","from xml.etree.ElementTree import Element, ElementTree\n","\n","import time                \n","from collections import OrderedDict # 라벨 dictionary를 만들 때 필요\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYKcp7AEvDHi"},"outputs":[],"source":["# box score threshold 0.05 -> 0.1\n","\n","def build_model(class_n):\n","    backbone = torchvision.models.vgg16(pretrained=True).features[:-1]\n","    backbone_out = 512\n","    backbone.out_channels = backbone_out\n","\n","    anchor_generator = torchvision.models.detection.rpn.AnchorGenerator(sizes=((128, 256, 512),),aspect_ratios=((0.5, 1.0, 2.0),))\n","\n","    resolution = 7\n","    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=resolution, sampling_ratio=2)\n","\n","    box_head = torchvision.models.detection.faster_rcnn.TwoMLPHead(in_channels= backbone_out*(resolution**2),representation_size=4096) \n","    box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(4096, class_n+1) # class + 배경 1. \n","\n","    model = torchvision.models.detection.FasterRCNN(backbone, num_classes=None,\n","                       min_size = 600, max_size = 1000,\n","                       rpn_anchor_generator=anchor_generator,\n","                       rpn_pre_nms_top_n_train = 6000, rpn_pre_nms_top_n_test = 6000,\n","                       rpn_post_nms_top_n_train=2000, rpn_post_nms_top_n_test=300,\n","                       rpn_nms_thresh=0.7,rpn_fg_iou_thresh=0.7,  rpn_bg_iou_thresh=0.3,\n","                       rpn_batch_size_per_image=256, rpn_positive_fraction=0.5,\n","                       box_roi_pool=roi_pooler, box_head = box_head, box_predictor = box_predictor,\n","                       box_score_thresh=0.05, box_nms_thresh=0.7, box_detections_per_img=300,\n","                       box_fg_iou_thresh=0.5, box_bg_iou_thresh=0.5,\n","                       box_batch_size_per_image=128, box_positive_fraction=0.25\n","                     )\n","    #roi head 있으면 num_class = None으로 함\n","    \n","    # weight parameter initialize\n","    for param in model.rpn.parameters():\n","        torch.nn.init.normal_(param,mean = 0.0, std=0.01)\n","\n","    for name, param in model.roi_heads.named_parameters():\n","        if \"bbox_pred\" in name:\n","            torch.nn.init.normal_(param,mean = 0.0, std=0.001)\n","        elif \"weight\" in name:\n","            torch.nn.init.normal_(param,mean = 0.0, std=0.01)\n","        if \"bias\" in name:\n","            torch.nn.init.zeros_(param)\n","    \n","    from torchsummary import summary\n","    summary(backbone,(3,600,1000), device = 'cpu')\n","    print(model)\n","    \n","    return model"]},{"cell_type":"code","source":["# label 준비\n","label_to_id = { #'1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, \\\n","               'ㄱ': 1, 'ㄴ': 2, 'ㄷ': 3, 'ㄹ': 13, 'ㅁ': 14, 'ㅂ': 15, 'ㅅ': 16, 'ㅇ': 17, \\\n","               'ㅈ': 18, 'ㅊ': 19, 'ㅋ': 20, 'ㅌ': 21, 'ㅍ': 22, 'ㅎ': 23, \\\n","               'ㅏ': 24, 'ㅐ': 25, 'ㅑ': 26, 'ㅒ': 27, 'ㅓ': 28, 'ㅔ': 29, 'ㅕ': 30, \\\n","               'ㅖ': 31, 'ㅗ': 32, 'ㅚ': 33, 'ㅛ': 34, 'ㅜ': 35, 'ㅟ': 36, 'ㅠ': 37, 'ㅡ': 38, 'ㅢ': 39, 'ㅣ': 40}\n","\n","id_to_label = {}\n","cnt = 1\n","for k, v in label_to_id.items():\n","    #id_to_label[str(v)] = k\n","    id_to_label[str(cnt)] = k\n","    cnt+=1\n","\n","display(id_to_label)\n","\n","print(len(id_to_label))"],"metadata":{"id":"Dm5I11tPGuxS","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c92ba2b2-46a2-4a8b-e48c-ed5be1b2ee65","executionInfo":{"status":"ok","timestamp":1686567299435,"user_tz":-540,"elapsed":19,"user":{"displayName":"­강영신 | 산업데이터엔지니어링학과 | 한양대(서울)","userId":"09673389676227113106"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["{'1': 'ㄱ',\n"," '2': 'ㄴ',\n"," '3': 'ㄷ',\n"," '4': 'ㄹ',\n"," '5': 'ㅁ',\n"," '6': 'ㅂ',\n"," '7': 'ㅅ',\n"," '8': 'ㅇ',\n"," '9': 'ㅈ',\n"," '10': 'ㅊ',\n"," '11': 'ㅋ',\n"," '12': 'ㅌ',\n"," '13': 'ㅍ',\n"," '14': 'ㅎ',\n"," '15': 'ㅏ',\n"," '16': 'ㅐ',\n"," '17': 'ㅑ',\n"," '18': 'ㅒ',\n"," '19': 'ㅓ',\n"," '20': 'ㅔ',\n"," '21': 'ㅕ',\n"," '22': 'ㅖ',\n"," '23': 'ㅗ',\n"," '24': 'ㅚ',\n"," '25': 'ㅛ',\n"," '26': 'ㅜ',\n"," '27': 'ㅟ',\n"," '28': 'ㅠ',\n"," '29': 'ㅡ',\n"," '30': 'ㅢ',\n"," '31': 'ㅣ'}"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["31\n"]}]},{"cell_type":"code","source":["# https://www.youtube.com/watch?v=YjWh7QvVH60\n","\n","# import dependencies\n","from IPython.display import display, Javascript, Image\n","from google.colab.output import eval_js\n","from base64 import b64decode, b64encode\n","import cv2\n","import numpy as np\n","import PIL\n","import io\n","import html\n","import time"],"metadata":{"id":"0dcU-r18H8IC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image\n","def js_to_image(js_reply):\n","  \"\"\"$\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"OP9H5si4Immd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    \n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      labelElement.style.fontSize = '30px';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<span style=\"color: red; font-weight: bold;\">' +\n","          'When finished, click here or on the video to stop this demo</span>';\n","      div.appendChild(instruction);\n","      instruction.onclick = () => { shutdown = true; };\n","      \n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result};\n","    }\n","    ''')\n","\n","  display(js)\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"metadata":{"id":"Z4Zlio-BIqHF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 불러오기\n","- 미리 저장해 둔 모델 파일을 불러오기 위해 구글 드라이브에 마운트 해 줍니다.\n","- gpu를 연결합니다.\n","- 사전 세팅에서 작성한 모델 구현 함수를 통해 모델을 구현합니다. \n","- 학습된 가중치를 구현한 모델에 불러옵니다. \n"],"metadata":{"id":"H_jeSX1pE9h2"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2hLO2TzTFkE_","outputId":"3fd7ba29-9465-4a70-d4d7-15d7cb3c39db","executionInfo":{"status":"ok","timestamp":1686567318255,"user_tz":-540,"elapsed":18833,"user":{"displayName":"­강영신 | 산업데이터엔지니어링학과 | 한양대(서울)","userId":"09673389676227113106"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# model 저장 경로에 따라 달라짐 ** 개인적 drive 상황 확인 필요 \n","!cp '/content/drive/MyDrive/Colab Notebooks/model_vgg16_01.pth' '.'"],"metadata":{"id":"NxIxt4hiFH_E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686567318255,"user_tz":-540,"elapsed":10,"user":{"displayName":"­강영신 | 산업데이터엔지니어링학과 | 한양대(서울)","userId":"09673389676227113106"}},"outputId":"a479a0fb-6150-4e52-f2bc-dae6f7dc11fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cp: cannot stat '/content/drive/MyDrive/Colab Notebooks/ Master-2023/인공지능1/model_vgg16_01.pth': No such file or directory\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdhh__QxmcM_","outputId":"7fd625df-7630-405d-e697-560a287a420e","executionInfo":{"status":"ok","timestamp":1686567318256,"user_tz":-540,"elapsed":5,"user":{"displayName":"­강영신 | 산업데이터엔지니어링학과 | 한양대(서울)","userId":"09673389676227113106"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["현재 사용중인 device:  cpu\n"]}],"source":["#GPU연결\n","if torch.cuda.is_available():\n","    device = torch.device('cuda:0')\n","    \n","else:\n","    device = torch.device('cpu')\n","\n","print('현재 사용중인 device: ', device)"]},{"cell_type":"code","source":["import warnings\n","\n","warnings.filterwarnings(action='ignore')\n","\n","print('build a model...')\n","print('backbone is vgg16')\n","\n","class_n = 31\n","model = build_model(class_n)\n","\n","print('model building is done!')\n","\n","\n","# load_fine-tuned_model\n","check_point = torch.load(\"./model_vgg16_01.pth\", map_location=device) \n","model.load_state_dict(check_point)\n","print('loaded fine-tuned model')\n","\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GMoN7ll7E_2c","outputId":"4596aba8-7d7d-40c7-b595-2172cea66cba","executionInfo":{"status":"error","timestamp":1686567340087,"user_tz":-540,"elapsed":21835,"user":{"displayName":"­강영신 | 산업데이터엔지니어링학과 | 한양대(서울)","userId":"09673389676227113106"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["build a model...\n","backbone is vgg16\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:03<00:00, 177MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1        [-1, 64, 600, 1000]           1,792\n","              ReLU-2        [-1, 64, 600, 1000]               0\n","            Conv2d-3        [-1, 64, 600, 1000]          36,928\n","              ReLU-4        [-1, 64, 600, 1000]               0\n","         MaxPool2d-5         [-1, 64, 300, 500]               0\n","            Conv2d-6        [-1, 128, 300, 500]          73,856\n","              ReLU-7        [-1, 128, 300, 500]               0\n","            Conv2d-8        [-1, 128, 300, 500]         147,584\n","              ReLU-9        [-1, 128, 300, 500]               0\n","        MaxPool2d-10        [-1, 128, 150, 250]               0\n","           Conv2d-11        [-1, 256, 150, 250]         295,168\n","             ReLU-12        [-1, 256, 150, 250]               0\n","           Conv2d-13        [-1, 256, 150, 250]         590,080\n","             ReLU-14        [-1, 256, 150, 250]               0\n","           Conv2d-15        [-1, 256, 150, 250]         590,080\n","             ReLU-16        [-1, 256, 150, 250]               0\n","        MaxPool2d-17         [-1, 256, 75, 125]               0\n","           Conv2d-18         [-1, 512, 75, 125]       1,180,160\n","             ReLU-19         [-1, 512, 75, 125]               0\n","           Conv2d-20         [-1, 512, 75, 125]       2,359,808\n","             ReLU-21         [-1, 512, 75, 125]               0\n","           Conv2d-22         [-1, 512, 75, 125]       2,359,808\n","             ReLU-23         [-1, 512, 75, 125]               0\n","        MaxPool2d-24          [-1, 512, 37, 62]               0\n","           Conv2d-25          [-1, 512, 37, 62]       2,359,808\n","             ReLU-26          [-1, 512, 37, 62]               0\n","           Conv2d-27          [-1, 512, 37, 62]       2,359,808\n","             ReLU-28          [-1, 512, 37, 62]               0\n","           Conv2d-29          [-1, 512, 37, 62]       2,359,808\n","             ReLU-30          [-1, 512, 37, 62]               0\n","================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 6.87\n","Forward/backward pass size (MB): 2607.89\n","Params size (MB): 56.13\n","Estimated Total Size (MB): 2670.89\n","----------------------------------------------------------------\n","FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(600,), max_size=1000, mode='bilinear')\n","  )\n","  (backbone): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(512, 9, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=25088, out_features=4096, bias=True)\n","      (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=4096, out_features=32, bias=True)\n","      (bbox_pred): Linear(in_features=4096, out_features=128, bias=True)\n","    )\n","  )\n",")\n","model building is done!\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fab179353377>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# load_fine-tuned_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcheck_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model_vgg16_01.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loaded fine-tuned model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model_vgg16_01.pth'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_3Uc-Jv3GOmc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Application\n","- signLang Translation\n","- video를 촬영하며 영상 속 지어(수어)를 번역하여 하단에 출력합니다."],"metadata":{"id":"cJjIcoTEH8wL"}},{"cell_type":"code","source":["!cp '/content/drive/MyDrive/Colab Notebooks/ Master-2023/인공지능1/signLang/AppleGothic.ttf' '.'"],"metadata":{"id":"4Wc_CqBgKyEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp '/content/drive/MyDrive/Colab Notebooks/ Master-2023/인공지능1/signLang/unicode.py' '.'"],"metadata":{"id":"DjM62jj-UMAk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from unicode import join_jamos ,split_syllable_char"],"metadata":{"id":"e_JPNEVADh1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_repeated_letters(word):\n","    tmp_result = ''\n","    for i in range(len(word)):\n","        if i == 0 or word[i] != word[i-1]:\n","            tmp_result += word[i]\n","\n","    tmp_result = join_jamos (tmp_result)\n","\n","    result = \"\"\n","    for i in tmp_result :\n","        r_data = split_syllable_char(i)\n","        count = r_data.count(None)\n","        if count != 2 :\n","            result+= i\n","\n","    return result"],"metadata":{"id":"TAAsG9spDhV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image, ImageFont, ImageDraw\n","\n","# start streaming video from webcam\n","video_stream()\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","count = 0 \n","txt =''\n","\n","while True:\n","    js_reply = video_frame(label_html, bbox)\n","    if not js_reply:\n","        break\n","\n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","\n","    # create transparent overlay for bounding box\n","    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n","\n","\n","    frame = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    image_np = np.array(frame)\n","\n","    to_tensor = T.ToTensor()\n","    input_tensor = to_tensor(image_np).unsqueeze(0) #dtype float32\n","\n","    # inference\n","    model.eval()\n","\n","    with torch.no_grad():\n","        predict = model(input_tensor.to(device))\n","\n","    boxes, labels = predict[0]['boxes'], predict[0]['labels']\n","    \n","    \n","\n","    try:\n","        # threshold 부여\n","        idx = torch.argmax(predict[0]['scores']).detach().cpu().numpy()\n","        score = predict[0][\"scores\"][idx].detach().cpu().numpy()\n","\n","        # predict 결과 50 점 이상인 경우에 한해서만 box 처리 진행\n","        if score >= 0.5 : \n","          box, label = boxes[idx].detach().cpu().numpy(), labels[idx].detach().cpu().numpy()\n","          # bbox coordinate\n","          xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]\n","        else :\n","          label= \"\"\n","          xmin, ymin, xmax, ymax  = 0, 0, 0, 0\n","\n","        # emtpy box drawing by PIL\n","        #  1. font setting for PIL\n","        font_size = 40\n","        font = ImageFont.truetype(\"AppleGothic.ttf\", font_size)\n","        \n","    except:\n","        label= \"\"\n","        xmin, ymin, xmax, ymax  = 0, 0, 0, 0\n","        pass\n","\n","        #  2. PIL Draw \n","    bbox_pil = Image.fromarray(np.uint8(bbox_array)) # .convert('RGB')\n","    draw = ImageDraw.Draw(bbox_pil)\n","    draw.rectangle((xmin, ymin, xmax, ymax), outline=(0,255,0), width=4)\n","    if label != \"\" :\n","      #print(label)\n","      draw.text((xmin, ymin-5), id_to_label[str(label)], fill=(0,255,0), font=font)\n","    bbox_array = np.array(bbox_pil)\n","\n","  \n","   \n","    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n","\n","    # convert overlay of bbox into bytes\n","    bbox_bytes = bbox_to_bytes(bbox_array)\n","    \n","    # update bbox so next frame gets new overlay\n","    bbox = bbox_bytes\n","    if label != \"\" :\n","      txt += id_to_label[str(label)]\n","    \n","    result_txt=join_jamos(remove_repeated_letters(txt))\n","    if len(result_txt) > 10 :\n","      result_txt = \"\"\n","      txt = \"\"\n","    label_html =result_txt"],"metadata":{"id":"3nzY0s-bIvhE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vqv8OSS1YWLW"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}